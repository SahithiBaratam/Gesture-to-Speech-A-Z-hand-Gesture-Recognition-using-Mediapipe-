# Gesture-to-Speech(A-Z-hand-Gesture-Recognition-using-Mediapipe)
A real-time gesture recognition and speech generation system designed for non-verbal patients. This project detects Aâ€“Z hand gestures using MediaPipe, converts them into text, and finally produces speech output.
The system is lightweight, accurate, and runs completely offline.

ğŸš€ Features
âœ” Real-time gesture recognition using MediaPipe Hands
âœ” Classifies Aâ€“Z alphabets
âœ” Builds sentences from detected characters
âœ” Converts text to speech using pyttsx3
âœ” Offline â€” no internet required
âœ” Fast and lightweight (no TensorFlow needed)
âœ” Works on any background and lighting conditions
âœ” Easy to train with your own gesture samples

ğŸ“Œ Project Structure
Gesture_Mediapipe/
â”‚
â”œâ”€â”€ data/
â”‚     â””â”€â”€ mp_landmarks.csv             # Auto-generated gesture dataset
â”‚
â”œâ”€â”€ models/
â”‚     â”œâ”€â”€ mp_model.pkl                 # Trained RandomForest model
â”‚     â””â”€â”€ mp_label_encoder.pkl         # Label encoder for Aâ€“Z
â”‚
â”œâ”€â”€ src/
â”‚     â”œâ”€â”€ collect_mediapipe_data.py    # Collect hand gesture samples
â”‚     â”œâ”€â”€ train_mediapipe_model.py     # Train the ML model
â”‚     â”œâ”€â”€ live_mediapipe_app.py        # Main real-time recognition app
â”‚     â”œâ”€â”€ sentence_builder.py          # Handles sentence construction
â”‚     â”œâ”€â”€ tts_engine.py                # Text-to-Speech engine (multiple speak fix)
â”‚
â”œâ”€â”€ venv/                               # Virtual environment (not uploaded)
â”‚
â””â”€â”€ README.md

ğŸ›  Installation
1ï¸âƒ£ Clone the repository
git clone <your-repo-link>
cd Gesture_Mediapipe
2ï¸âƒ£ Create and activate virtual environment
python -m venv venv
venv\Scripts\activate    # Windows
3ï¸âƒ£ Install dependencies
pip install -r requirements.txt


ğŸ§ª Step 1 â€” Collect Gesture Data
cd src
python collect_mediapipe_data.py
Controls:
| Action                   | Key                     |
| ------------------------ | ----------------------- |
| Save sample for a letter | Press that letter (Aâ€“Z) |
| Quit                     | **ESC**                 |
ğŸ‘‰ Collect at least 20â€“40 samples per letter for good accuracy.
A file will be created:
data/mp_landmarks.csv

ğŸ§  Step 2 â€” Train the Model
python train_mediapipe_model.py
Outputs:
models/mp_model.pkl
models/mp_label_encoder.pkl

ğŸ¤ Step 3 â€” Run the Real-Time Gesture-to-Speech App
python live_mediapipe_app.py
| Action              | Key       |
| ------------------- | --------- |
| Add detected letter | **A**     |
| Add space           | **SPACE** |
| Backspace           | **B**     |
| Clear sentence      | **C**     |
| Speak full sentence | **S**     |
| Exit application    | **ESC**   |

ğŸ§  How It Works
1. MediaPipe Hands extracts 21 hand landmarks
2. Landmarks are flattened into a 63-point feature vector
3. A RandomForest model predicts the gesture (Aâ€“Z)
4. The predicted letter is added to your live sentence
5. Sentence is spoken out loud via pyttsx3


âœ¨ Advantages
Offline
Fast prediction
Works with any webcam
Easy to extend (digits, words, custom gestures)
No deep learning required


ğŸš§ Future Enhancements
Tkinter GUI interface
Dynamic gesture support
Predefined common phrases (â€œHelp meâ€, â€œWaterâ€, etc.)
Multilingual speech output
Android version using MediaPipe + TFLite


ğŸ¤ Contributing
Contributions are welcome!
You may:
Add new gestures
Improve UI
Enhance accuracy
Document the project


ğŸ“œ License
This project is open source.
You may modify and reuse it for academic or personal use.


â¤ï¸ Acknowledgements
1. MediaPipe by Google
2. OpenCV
3. scikit-learn
4. pyttsx3

<img width="794" height="641" alt="Screenshot 2025-11-28 080709" src="https://github.com/user-attachments/assets/e4ca8c36-375b-434b-9be8-379b5a27f9f6" />
<img width="795" height="634" alt="Screenshot 2025-11-28 080647" src="https://github.com/user-attachments/assets/9ced98f2-ecfb-4485-895b-d6c517d8196d" />

